---
phase: 36-performance-tuning
plan: 01
type: execute
---

<objective>
Ottimizzare performance WebSocket real-time sync: load testing, latency optimization (<100ms average), stress testing, bottleneck identification.

Purpose: Garantire che il sistema WebSocket real-time sia production-ready con performance eccellenti sotto carico (10-50+ concurrent users), identificare e risolvere bottlenecks prima del deployment finale.
Output: Load test scripts, performance optimizations, stress test report, latency migliorata (<100ms average), sistema scalabile validato.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Auto-selected based on dependency graph:
@.planning/phases/34-e2e-testing-multidevice/34-01-SUMMARY.md
@.planning/phases/35-monitoring-observability/35-01-SUMMARY.md

# Key files from frontmatter:
@archibald-web-app/backend/src/websocket-server.ts
@archibald-web-app/backend/src/index.ts
@archibald-web-app/frontend/e2e/helpers/multi-device.ts

**Tech stack available:** @playwright/test@1.58.1, ws library, Express health endpoints, React monitoring patterns
**Established patterns:**
- WebSocket connection pool Map<userId, Set<WebSocket>>
- Ping/pong heartbeat 30s interval
- Rolling average latency (100 samples)
- Multi-device E2E testing with Playwright
- IndexedDB state verification
- Metrics tracking (averageLatency, messagesSent, reconnectionCount, uptime)

**Constraining decisions:**
- Phase 35: Latency threshold 100ms per badge color (green ≤100ms, orange >100ms) - questo è il target production
- Phase 34: Relaxed latency assertions for E2E (5s timeout) - test overhead documented
- Phase 29: Ping/pong heartbeat 30s interval - industry standard, può essere tuned
- Phase 29: Connection pool Map<userId, Set<WebSocket>> - efficient O(1) lookup

**Issues being addressed:** None (preventive optimization pre-production deployment)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Load Testing Infrastructure with K6</name>
  <files>archibald-web-app/load-tests/websocket-load.js, archibald-web-app/load-tests/package.json, archibald-web-app/load-tests/README.md</files>
  <action>
  Installare K6 load testing tool (https://k6.io/) come dev dependency:
  - Creare directory archibald-web-app/load-tests/
  - Setup package.json con script "k6": "k6 run websocket-load.js"
  - Installare k6 via npm install -D k6 (o documentare installazione locale: brew install k6)

  Creare websocket-load.js script:
  - Simulare 10-50 concurrent WebSocket connections (VUs - virtual users)
  - Ogni VU: autentica con JWT (mock o test user), connette a ws://localhost:3000/ws/realtime, invia/riceve messaggi DRAFT_CREATED/UPDATED/DELETED
  - Misurare: connection time, message round-trip latency, reconnection success rate, memory usage
  - Durata: 5 min ramp-up + 10 min sustained load + 5 min ramp-down
  - Thresholds: averageLatency <100ms (p95), connection success >99%, no memory leaks

  AVOID: Non usare Artillery (meno supporto WebSocket), non usare JMeter (troppo complesso). K6 è industry standard per WebSocket load testing con ottima DX.

  Creare README.md con:
  - Prerequisiti (K6 installazione, backend running)
  - Comandi esecuzione (k6 run --vus 10 --duration 5m websocket-load.js)
  - Interpretazione metriche (latency percentiles, throughput, error rate)
  - Expected baselines (10 users: <50ms p95, 50 users: <100ms p95)
  </action>
  <verify>
  cd archibald-web-app/load-tests && k6 run --vus 5 --duration 30s websocket-load.js completes without errors, mostra metriche (latency, connections, messages)
  </verify>
  <done>
  K6 script funzionante, simula multi-user WebSocket load, metriche key tracked (latency p95/p99, connection success rate, message throughput), README documentation completa
  </done>
</task>

<task type="auto">
  <name>Task 2: Latency Optimization & Broadcast Performance</name>
  <files>archibald-web-app/backend/src/websocket-server.ts, archibald-web-app/backend/src/index.ts</files>
  <action>
  Analizzare current latency baseline da Phase 35 WebSocketMonitor:
  - Check admin dashboard http://localhost:3000/admin per averageLatency corrente
  - Run K6 load test (Task 1) per identificare latency under load (10/20/50 users)

  Ottimizzazioni target (applicare SOLO se latency >100ms o degradation sotto load):

  1. **Broadcast optimization** (se latency alta durante broadcast):
     - Batch broadcasts: invece di JSON.stringify() per ogni connection, stringify UNA VOLTA e riutilizzare buffer
     - Parallel send: usare Promise.all() invece di forEach sequenziale per ws.send()
     - Esempio: const message = JSON.stringify(event); Promise.all([...userConnections].map(ws => ws.send(message)))

  2. **Connection pool optimization** (se lookup lento):
     - Attualmente Map<userId, Set<WebSocket>> è già O(1), unlikely bottleneck
     - Se necessario: pre-allocare Sets con size hint, evitare delete/re-add thrashing

  3. **Ping/pong tuning** (se heartbeat overhead alto):
     - Current: 30s interval (industry standard)
     - Se high connection count (50+): consider 45s or 60s interval
     - Trade-off: longer interval = less overhead ma slower zombie detection

  4. **Message serialization** (se CPU-bound):
     - Usare MessagePack invece di JSON (più compatto, più veloce)
     - Requires client+server changes, evaluate cost/benefit

  AVOID:
  - Non cambiare architettura singleton (già optimal)
  - Non introdurre clustering (premature, aggiunge complessità)
  - Non modificare connection pool structure (Map<Set> è already efficient)
  - NON ottimizzare senza misurare PRIMA (no premature optimization)

  Strategia: Run K6 load test BEFORE e AFTER ogni optimization, compare latency p95/p99. Se nessun miglioramento measurable, revert change.
  </action>
  <verify>
  K6 load test mostra latency p95 <100ms per 10-20 concurrent users, p99 <150ms, no degradation durante sustained load, admin dashboard conferma averageLatency <100ms
  </verify>
  <done>
  Latency optimizations implementate con measurable improvements, K6 test results documented (before/after), target <100ms p95 achieved per 10-20 users, admin monitoring conferma performance stabile
  </done>
</task>

<task type="auto">
  <name>Task 3: Stress Testing, Memory Profiling & Bottleneck Report</name>
  <files>archibald-web-app/load-tests/stress-test.js, archibald-web-app/load-tests/PERFORMANCE-REPORT.md, archibald-web-app/backend/src/websocket-server.ts</files>
  <action>
  Creare stress-test.js (K6 script) per push limits:
  - Spike test: 0 → 50 users in 30s, hold 2 min, drop to 0
  - Soak test: 20 users sustained for 30 min (detect memory leaks)
  - Breakpoint test: gradually increase users fino a failure (10→20→30→40→50+)
  - Misurare: connection failures, message loss, memory growth, CPU usage

  Memory profiling durante stress test:
  - Node.js heap snapshots: node --inspect backend/src/index.ts, Chrome DevTools Memory tab
  - Monitor process.memoryUsage() ogni 30s durante soak test
  - Verificare: metrics.latencySamples array bounded (max 100 - Phase 35), no unbounded growth in connectionPool
  - Check for: event listener leaks, setTimeout/setInterval leaks, large object retention

  Bottleneck identification:
  - Use K6 metrics + Node.js profiler per identificare hot paths
  - Check CPU profiling: flame graphs per broadcast(), registerConnection(), authenticateConnection()
  - Identificare: slow JWT verification, slow JSON.stringify, slow ws.send(), database queries (if any)

  Creare PERFORMANCE-REPORT.md con:
  - **Baseline Performance**: 10 users (latency, throughput, memory), 20 users, 50 users
  - **Bottlenecks Identified**: Top 3 performance hotspots con profiling data
  - **Optimizations Applied**: Before/after metrics per ogni optimization (Task 2)
  - **Scalability Limits**: Max concurrent users before degradation, memory usage trends
  - **Recommendations**: Next steps per ulteriore scaling (clustering, Redis pub/sub, load balancer)
  - **Production Readiness**: ✅/❌ checklist (latency target met, no memory leaks, stress test passed)

  AVOID:
  - Non introdurre external dependencies senza strong justification (Redis, message queues)
  - Non over-engineer per 100+ users se current usage <20 (pragmatic scaling)
  - Non ignorare memory leaks anche se piccoli (production long-running processes)
  </action>
  <verify>
  Stress test completa senza crash, soak test mostra memoria stabile (no leaks), PERFORMANCE-REPORT.md documenta baseline + bottlenecks + optimizations + production readiness assessment
  </verify>
  <done>
  Stress testing completo con spike/soak/breakpoint tests, memory profiling verificato (no leaks, bounded growth), bottlenecks documentati, PERFORMANCE-REPORT.md comprehensive, production readiness assessment positivo (latency <100ms, scalabile 20-30 users, no critical issues)
  </done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] K6 load test infrastructure funzionante (websocket-load.js eseguibile)
- [ ] Latency p95 <100ms per 10-20 concurrent users (K6 metrics)
- [ ] Stress test passed senza crash o memory leaks
- [ ] PERFORMANCE-REPORT.md completo con baseline, bottlenecks, optimizations, recommendations
- [ ] Admin dashboard conferma averageLatency <100ms dopo optimizations
- [ ] No regressions in existing E2E tests (npm run test:e2e passa)
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- K6 load testing infrastructure operativa
- Latency target <100ms p95 achieved per 10-20 users
- Stress testing validates scalability (20-30 users sustained)
- Memory profiling conferma no leaks
- PERFORMANCE-REPORT.md documenta performance baseline e readiness
- Bottlenecks identified e optimizations measurably effective
- Production readiness assessment positivo
  </success_criteria>

<output>
After completion, create `.planning/phases/36-performance-tuning/36-01-SUMMARY.md`:

# Phase 36 Plan 01: Performance Tuning & Optimization Summary

**WebSocket real-time sync performance optimization: load testing, latency <100ms, stress testing, bottleneck elimination**

## Accomplishments

- K6 load testing infrastructure con multi-user WebSocket simulation
- Latency optimizations: [list specific optimizations applied]
- Stress testing: spike/soak/breakpoint tests passed
- Memory profiling: no leaks detected, bounded growth verified
- PERFORMANCE-REPORT.md: comprehensive baseline + bottlenecks + recommendations
- Production readiness: ✅ latency target met, scalability validated

## Files Created/Modified

- `archibald-web-app/load-tests/websocket-load.js` - K6 load test script
- `archibald-web-app/load-tests/stress-test.js` - K6 stress test script
- `archibald-web-app/load-tests/PERFORMANCE-REPORT.md` - Performance analysis
- `archibald-web-app/backend/src/websocket-server.ts` - Latency optimizations (if any)

## Decisions Made

[Documentare decisioni chiave su optimization trade-offs, scaling limits, production thresholds]

## Issues Encountered

[Documentare problemi durante load testing, memory leaks trovati e fixati, optimizations che non hanno funzionato]

## Next Phase Readiness

✅ **Ready for Production Deployment (Milestone v3.0 complete)**

### What's ready:
- ✅ WebSocket real-time sync production-ready
- ✅ Latency <100ms validated under load
- ✅ Scalability tested (20-30 concurrent users)
- ✅ Memory leaks eliminated
- ✅ Performance monitoring dashboard operativo
- ✅ E2E tests passing

### What Production Deployment needs:
- VPS deployment configuration
- SSL/TLS certificates for wss://
- Load balancer setup (if scaling beyond 50 users)
- Production monitoring alerts (latency spikes, memory usage)
- Backup/disaster recovery plan
</output>
