---
phase: 15-individual-sync-testing
plan: 03
subsystem: sync
tags: [data-integrity, checkpoint-system, crash-recovery, early-termination]
---

# Phase 15 Plan 03: Data Integrity & Resilience Testing

**Test checkpoint system, crash recovery, and data integrity guarantees for all 4 sync types**

## Objective

Execute data integrity tests (D-1 through D-3) to validate resilience mechanisms:

1. **D-1 (HIGH)**: Checkpoint crash recovery (resume from page 150/300)
2. **D-2 (CRITICAL)**: Concurrent write data integrity (no lost updates)
3. **D-3 (MEDIUM)**: Order sync early termination (30-day lookback)

**Goal**: Verify that sync system preserves data integrity under failure scenarios and edge cases.

**Non-goal**: Fixing integrity issues (deferred to Phase 16 if found).

## Execution Context

**Testing approach**:
- Simulated failure scenarios (kill process, network interruption)
- Real production code paths (no mocks)
- Verify database state consistency after failures
- Validate checkpoint and early termination logic

**Environment**:
- Development environment with full Archibald backend running
- Real SQLite databases (with checkpoints enabled)
- Real Puppeteer browser sessions
- Simulated crash scenarios (SIGKILL, network disconnect)

**Success criteria**:
- Checkpoint system validated (crash recovery works)
- Data integrity guaranteed (no lost updates, no partial writes)
- Early termination logic validated (order sync 30-day lookback)
- Failure modes documented with recovery procedures

## Context

**From Phase 14 findings**:

### Checkpoint System (3-Day Threshold)

All 4 syncs use checkpoint system for crash recovery:

```typescript
// Before scraping page N
await this.saveCheckpoint(userId, pageNumber, totalPages);

// If sync crashes, next sync resumes from last checkpoint
const lastCheckpoint = await this.loadCheckpoint(userId);
if (lastCheckpoint && isWithin3Days(lastCheckpoint.timestamp)) {
  startPage = lastCheckpoint.pageNumber;
}
```

**Critical behavior**:
- Checkpoint saved before each page scrape (progressive writes)
- 3-day threshold: resume if crash within 3 days, else start fresh
- Per-user checkpoints (customers-{userId}.checkpoint.json)
- Shared checkpoints (products.checkpoint.json, prices.checkpoint.json)

**Unknowns**:
- Does checkpoint system actually work? (never tested)
- What happens if checkpoint file is corrupted?
- Is 3-day threshold appropriate? (too short, too long?)

### Transaction-Based Writes (Atomicity)

Price sync uses transaction-based batch updates:

```typescript
await this.db.transaction(async (trx) => {
  for (const price of pagePrices) {
    await trx.updatePrice(price);
    await trx.logPriceChange(price.oldPrice, price.newPrice);
  }
});
```

**Critical behavior**:
- All-or-nothing writes per page (atomic)
- Audit logging included in transaction
- Concurrent writes to same table (product + price) need testing

**Unknowns**:
- Are transactions actually atomic? (SQLite guarantees)
- What happens if transaction fails mid-batch?
- Concurrent write contention (from Phase 15-01 testing)

### Order Sync Early Termination

Order sync uses intelligent lookback with early termination:

```typescript
// Incremental sync: scrape from 30 days before oldest order
const oldestOrder = await this.db.getOldestOrder(userId);
const startDate = subDays(oldestOrder.date, 30);

// Early termination: stop after 2 consecutive pages with only old orders
if (consecutiveOldPages >= 2) {
  break; // All new orders found
}
```

**Critical behavior**:
- First sync: scrape from January 1st of current year (full year)
- Incremental sync: 30-day lookback from oldest order
- Early termination: 2 consecutive pages with old orders
- Cache-first + lazy sync (10-minute threshold vs 2-hour login threshold)

**Unknowns**:
- Does early termination work correctly? (avoid re-scraping old data)
- Is 30-day lookback sufficient? (data freshness)
- What if orders are out of chronological order? (pagination issues)

### Key Files

**Checkpoint System**:
- `archibald-web-app/backend/src/services/checkpoint-manager.ts` - Checkpoint save/load logic
- `archibald-web-app/backend/src/services/customer-sync-service.ts` - Checkpoint integration
- `archibald-web-app/backend/src/services/product-sync-service.ts` - Checkpoint integration

**Transaction System**:
- `archibald-web-app/backend/src/db/product-db.ts` - Transaction-based updates
- `archibald-web-app/backend/src/db/price-db.ts` - Batch update transactions

**Early Termination**:
- `archibald-web-app/backend/src/services/order-history-service.ts` - Early termination logic

## Tasks

### Task 1: Test D-1 (Checkpoint Crash Recovery) ðŸ”´ HIGH

**What**: Simulate crash during product sync, verify resume from last checkpoint.

**Steps**:
1. Prepare test environment:
   - Clear products database (fresh sync)
   - Delete any existing checkpoint files
   - Enable detailed logging for product sync
2. Start product sync and crash mid-way:
   - Trigger full product sync (300 pages)
   - Monitor logs for page progress (wait until ~page 150/300)
   - Kill process (SIGKILL or manual stop) to simulate crash
   - Verify checkpoint file saved (products.checkpoint.json with page 150)
3. Resume sync and verify checkpoint recovery:
   - Restart backend (process recovered)
   - Trigger product sync again
   - Verify logs show "Resuming from checkpoint: page 150"
   - Verify sync completes (pages 150-300 scraped, not 1-300)
4. Verify database consistency:
   - Count products in database (should match page 150 + remaining pages)
   - Verify no duplicate products (checkpoint resume doesn't re-scrape)
5. Repeat test 3 times to verify consistency
6. Document findings:
   - Does checkpoint system work? (resume from correct page)
   - Database consistency preserved? (no duplicates, no missing data)
   - 3-day threshold appropriate? (test checkpoint expiration)

**Evidence to capture**:
- Console logs showing crash and resume (timestamps, page numbers)
- Checkpoint file contents (pageNumber, timestamp)
- Database row count before crash, after crash, after resume

**Verification**:
- [ ] Test executed 3 times with consistent results
- [ ] Checkpoint resume works (scraping resumes from correct page)
- [ ] Database consistency preserved (no duplicates, no data loss)
- [ ] 3-day threshold tested (expired checkpoints ignored)

**Commit**: `test(15-03): execute D-1 checkpoint crash recovery test`

---

### Task 2: Test D-2 (Concurrent Write Data Integrity) ðŸ”´ CRITICAL

**What**: Validate data integrity during concurrent product + price writes (from Phase 15-01 results).

**Steps**:
1. Review Phase 15-01 test results (C-1: Product + Price concurrent writes)
   - If C-1 showed data corruption â†’ HIGH PRIORITY verification
   - If C-1 showed success â†’ verify consistency is repeatable
2. Design data integrity verification test:
   - Run full product sync (insert all products with initial prices)
   - Run concurrent product + price sync (update same products)
   - Verify database state:
     - No lost product updates (name, category preserved)
     - No lost price updates (listPrice, priceA preserved)
     - No partial writes (row has all columns updated)
     - Audit log consistency (price_changes table matches products table)
3. Execute test 5 times to verify consistency
4. Verify transaction atomicity:
   - Inspect price_changes audit log
   - Verify all-or-nothing behavior (no partial page writes)
   - Check foreign key constraints (no orphaned price changes)
5. Document findings:
   - Is data integrity preserved during concurrent writes?
   - Are transactions atomic? (all-or-nothing per page)
   - Any evidence of lost updates or race conditions?

**Evidence to capture**:
- Database state before and after concurrent writes (row-level comparison)
- Audit log entries (price_changes table)
- Any errors or warnings in logs (lock contention, timeouts)

**Verification**:
- [ ] Test executed 5 times with consistent results
- [ ] Data integrity preserved (no lost updates, no partial writes)
- [ ] Transaction atomicity validated (all-or-nothing)
- [ ] Audit log consistency verified (matches products table)

**Commit**: `test(15-03): execute D-2 concurrent write integrity test`

---

### Task 3: Test D-3 (Order Sync Early Termination) ðŸŸ¡ MEDIUM

**What**: Validate order sync early termination logic (30-day lookback, 2 consecutive old pages).

**Steps**:
1. Prepare test environment:
   - Run full order sync for test user (establish baseline with old orders)
   - Database should have orders dating back 6+ months
   - Verify oldest order date in database
2. Simulate incremental sync scenario:
   - Trigger order sync again (should use 30-day lookback)
   - Monitor logs for:
     - Calculated start date (30 days before oldest order)
     - Pages scraped (should be fewer than full sync)
     - Early termination trigger (2 consecutive old pages)
3. Verify early termination behavior:
   - Check logs for "Early termination: 2 consecutive old pages"
   - Verify sync stopped before scraping all pages
   - Calculate pages saved (efficiency metric)
4. Verify data freshness:
   - Check database for new orders (if any added to Archibald)
   - Verify no missing orders (early termination didn't skip new data)
5. Test edge case: out-of-order pagination
   - If orders are not chronological, does early termination still work?
6. Document findings:
   - Does early termination work correctly?
   - Is 30-day lookback sufficient? (data freshness)
   - Pages saved by early termination (efficiency metric)
   - Any false positives? (terminated too early, missed new orders)

**Evidence to capture**:
- Console logs showing early termination logic (old page detection)
- Database state before and after incremental sync
- Pages scraped comparison (full sync vs incremental with early termination)

**Verification**:
- [ ] Test executed 3 times with consistent results
- [ ] Early termination works correctly (stops after 2 old pages)
- [ ] Data freshness preserved (no missing new orders)
- [ ] Efficiency metric calculated (pages saved)

**Commit**: `test(15-03): execute D-3 order early termination test`

---

### Task 4: Test Edge Cases (Checkpoint Expiration, Corrupted Checkpoint)

**What**: Validate checkpoint system handles edge cases gracefully.

**Steps**:
1. Test expired checkpoint (3-day threshold):
   - Create checkpoint file with timestamp 4 days ago
   - Trigger product sync
   - Verify logs show "Checkpoint expired, starting fresh sync"
   - Verify sync starts from page 1 (not resumed)
2. Test corrupted checkpoint file:
   - Create malformed checkpoint JSON (invalid syntax)
   - Trigger product sync
   - Verify error handling (graceful degradation)
   - Verify sync starts from page 1 (ignores corrupted checkpoint)
3. Test missing checkpoint file:
   - Delete checkpoint file
   - Trigger product sync
   - Verify sync starts from page 1 (no error)
4. Document findings:
   - Does checkpoint system handle edge cases gracefully?
   - Error handling robust? (no crashes, clear logs)
   - Fallback behavior correct? (start fresh if checkpoint invalid)

**Evidence to capture**:
- Console logs showing edge case handling (expired, corrupted, missing)
- Error messages (if any)
- Fallback behavior (start fresh sync)

**Verification**:
- [ ] Expired checkpoint test passed (starts fresh)
- [ ] Corrupted checkpoint test passed (graceful degradation)
- [ ] Missing checkpoint test passed (no error)
- [ ] Error handling robust (no crashes)

**Commit**: `test(15-03): execute checkpoint edge case tests`

---

### Task 5: Document Data Integrity Findings and Resilience Assessment

**What**: Create comprehensive data integrity report with resilience validation.

**Steps**:
1. Synthesize findings from Tasks 1-4
2. Create `15-03-INTEGRITY-REPORT.md` with structure:
   - Executive summary (resilience mechanisms validated)
   - D-1 Checkpoint Recovery results (crash recovery works)
   - D-2 Concurrent Write Integrity results (atomicity validated)
   - D-3 Early Termination results (efficiency validated)
   - Edge case handling (checkpoint expiration, corruption)
   - Resilience assessment (crash recovery, data consistency)
   - Issues found (if any) with severity and fix recommendations
3. Update issue tracker with data integrity findings
4. Define Phase 16 scope for integrity fixes (if needed)

**Verification**:
- [ ] Integrity report created with all test results
- [ ] Resilience mechanisms validated (checkpoint, transaction, early termination)
- [ ] Edge cases documented (error handling, fallback behavior)
- [ ] Phase 16 scope updated (if integrity issues found)

**Commit**: `docs(15-03): create data integrity and resilience report`

---

## Verification

After all tasks complete:

1. **Checkpoint Recovery (D-1)**: Crash recovery validated â†’ resilience HIGH
2. **Concurrent Write Integrity (D-2)**: Data consistency preserved â†’ SQLite atomicity works
3. **Early Termination (D-3)**: Efficiency validated â†’ intelligent sync working as intended
4. **Edge Cases**: Graceful degradation validated â†’ error handling robust

## Success Criteria

- [ ] D-1, D-2, D-3 tests executed 3+ times each (9+ total runs)
- [ ] Checkpoint system validated (crash recovery works)
- [ ] Data integrity guaranteed (no lost updates, atomic transactions)
- [ ] Early termination validated (order sync efficiency)
- [ ] Edge cases handled gracefully (expired, corrupted, missing checkpoints)
- [ ] Integrity report created with resilience assessment
- [ ] No code changes made (testing only, fixes deferred to Phase 16 if needed)

## Output

**Artifacts**:
- `.planning/phases/15-individual-sync-testing/15-03-INTEGRITY-REPORT.md` - Comprehensive data integrity report
- Checkpoint system validation (crash recovery works)
- Transaction atomicity validation (no lost updates)
- Early termination efficiency metrics (pages saved)
- Edge case handling documentation

**Knowledge gained**:
- Checkpoint system resilience (crash recovery validated)
- SQLite transaction atomicity (data consistency guaranteed)
- Order sync intelligent lookback (early termination efficiency)
- Error handling robustness (graceful degradation validated)
