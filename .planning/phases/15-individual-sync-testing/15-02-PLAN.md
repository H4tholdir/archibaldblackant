---
phase: 15-individual-sync-testing
plan: 02
subsystem: sync
tags: [performance-testing, baseline-metrics, delta-optimization, multi-user-concurrent]
---

# Phase 15 Plan 02: Performance Baseline Testing

**Establish performance baselines for all 4 sync types to inform optimization priorities in Phase 16+**

## Objective

Execute performance tests (P-1 through P-4) to establish empirical baselines:

1. **P-1**: Full customer sync (~5700 items, 230 pages)
2. **P-2**: Full product sync with image downloads (~6000 items, 300 pages)
3. **P-3**: Delta sync quick hash check (no changes detected)
4. **P-4**: Multi-user concurrent sync (3 users simultaneously)

**Goal**: Quantify actual performance characteristics to inform Phase 16+ optimization priorities.

**Non-goal**: Optimizing performance (deferred to Phase 16-18 based on baseline data).

## Execution Context

**Testing approach**:
- Manual user simulation + API calls
- Real production code paths (no mocks)
- Measure wall-clock time, database operations, resource usage
- Establish repeatable baselines (3 runs per test)

**Environment**:
- Development environment with full Archibald backend running
- Real SQLite databases (fresh state for full sync tests)
- Real Puppeteer browser sessions
- Simulated multi-user scenario (3 test users)

**Success criteria**:
- Baseline metrics established for all 4 tests
- Performance bottlenecks identified
- Optimization priorities ranked by impact
- Phase 16+ scope informed by empirical data

## Context

**From Phase 14 findings**:

### Expected Performance (Phase 14 Predictions)

| Sync Type | Items | Pages | Predicted Duration | Bottleneck |
|-----------|-------|-------|-------------------|------------|
| **Customers** | ~5,700 | ~230 | 15-20 min | Page scraping (3-5s/page) |
| **Products** | ~6,000 | ~300 | 15-20 min (or 8+ hours with images) | Image downloads |
| **Prices** | ~6,000 | ~300 | 15-20 min | Multi-level matching (50ms/item) |
| **Orders** | ~100-200 | ~10 | 5-10 min | Unified enrichment |

**Critical unknowns**:
- Are image downloads actually blocking product sync? (Issue 4 - HIGH)
- How efficient is delta sync quick hash? (saves ~3-4h/day predicted)
- What's the actual multi-user concurrent overhead? (isolation vs contention)

### Key Files

**Sync Services**:
- `archibald-web-app/backend/src/services/customer-sync-service.ts` - Customer sync implementation
- `archibald-web-app/backend/src/services/product-sync-service.ts` - Product sync + image downloads
- `archibald-web-app/backend/src/services/price-sync-service.ts` - Price sync + multi-level matching
- `archibald-web-app/backend/src/services/order-history-service.ts` - Order sync + unified enrichment

**Performance-Critical Code**:
- Product sync image download loop (lines 350-400 in product-sync-service.ts)
- Quick hash calculation (MD5 of first 10 items)
- Multi-level matching (ID â†’ name exact â†’ name normalized)
- Transaction batch updates

## Tasks

### Task 1: Test P-1 (Full Customer Sync) - Baseline Validation

**What**: Measure full customer sync duration to validate Phase 14 prediction (15-20 min).

**Steps**:
1. Prepare test environment:
   - Clear customer database for test user (fresh sync)
   - Verify Puppeteer browser pool is available
   - Enable detailed logging for customer sync
2. Execute full customer sync:
   - Trigger via manual API call or scheduler
   - Monitor logs for page scraping progress (timestamps)
   - Measure wall-clock duration (start to completion)
   - Count pages scraped and customers inserted
3. Repeat test 3 times to establish baseline
4. Analyze bottlenecks:
   - Time per page (scraping + parsing + DB insert)
   - Network latency (page load times)
   - Database write performance (batch vs individual)
5. Document findings:
   - Average duration (3 runs)
   - Items per minute rate
   - Bottleneck identification (scraping, parsing, or DB)
   - Comparison to Phase 14 prediction

**Evidence to capture**:
- Console logs with timestamps (page progress)
- Final metrics (duration, items scraped, pages processed)
- Resource usage (CPU, memory during sync)

**Verification**:
- [ ] Test executed 3 times with consistent results
- [ ] Baseline documented (average duration, std dev)
- [ ] Bottleneck identified
- [ ] Optimization priority ranked (low if 15-20min, high if >30min)

**Commit**: `test(15-02): establish P-1 customer sync baseline`

---

### Task 2: Test P-2 (Full Product Sync with Images) - Critical Bottleneck ðŸ”´

**What**: Measure full product sync duration to validate if image downloads block sync (Issue 4 - HIGH).

**Steps**:
1. Prepare test environment:
   - Clear products database (fresh sync)
   - Clear image cache directory
   - Enable detailed logging for product sync
2. Execute full product sync:
   - Trigger via manual API call
   - Monitor logs for page scraping AND image download progress
   - Measure wall-clock duration (start to completion)
   - Count pages scraped, products inserted, images downloaded
3. Repeat test 3 times to establish baseline
4. Analyze bottlenecks:
   - Time per page (scraping only)
   - Time per image download (blocking duration)
   - Total image download time vs scraping time
   - Confirm if image downloads are sequential and blocking
5. Document findings:
   - Average duration (3 runs)
   - Breakdown: scraping time vs image download time
   - Confirm Issue 4 severity (8+ hours if 6000 images @ 5s each?)
   - Optimization impact: how much faster without image downloads?

**Evidence to capture**:
- Console logs with timestamps (scraping progress, image download progress)
- Final metrics (duration, items scraped, images downloaded)
- Resource usage (network I/O, disk I/O)

**Verification**:
- [ ] Test executed 3 times with consistent results
- [ ] Baseline documented (scraping vs image download breakdown)
- [ ] Issue 4 severity confirmed (blocking or parallel?)
- [ ] Optimization priority: HIGH if image downloads block entire sync

**Commit**: `test(15-02): establish P-2 product sync + images baseline`

---

### Task 3: Test P-3 (Delta Sync Quick Hash) - Optimization Validation

**What**: Measure delta sync quick hash performance to validate ~3-4h/day savings claim.

**Steps**:
1. Prepare test environment:
   - Run full sync for all 4 types (establish baseline state)
   - Do NOT modify any data in Archibald (ensure quick hash detects "no changes")
2. Execute delta sync quick hash checks:
   - Trigger customer delta sync (should skip full sync)
   - Trigger product delta sync (should skip full sync)
   - Trigger price delta sync (should skip full sync)
   - Measure quick hash calculation duration for each
3. Repeat test 3 times for each sync type
4. Analyze efficiency:
   - Quick hash duration (<2 seconds predicted)
   - Skip rate validation (87% predicted)
   - Total time saved per day (3-4 hours predicted)
5. Document findings:
   - Average quick hash duration per sync type
   - Validation of skip rate (if no data changed)
   - Projected daily time savings (frequency Ã— skip rate Ã— full sync duration)

**Evidence to capture**:
- Console logs showing quick hash calculation and "no changes detected"
- Duration metrics for each quick hash check
- Validation of MD5 calculation efficiency

**Verification**:
- [ ] Test executed 3 times per sync type (12 total runs)
- [ ] Quick hash duration baseline established
- [ ] Skip rate validated (should be 100% for unchanged data)
- [ ] Time savings calculation confirmed

**Commit**: `test(15-02): establish P-3 delta sync quick hash baseline`

---

### Task 4: Test P-4 (Multi-User Concurrent) - Scalability Baseline

**What**: Measure multi-user concurrent sync performance to quantify isolation vs contention.

**Steps**:
1. Prepare test environment:
   - Create 3 test users (User A, User B, User C)
   - Clear databases for all 3 users (fresh sync)
   - Enable detailed logging for all sync services
2. Execute multi-user concurrent sync:
   - Trigger customer sync for all 3 users simultaneously (within 1 second)
   - Monitor logs for parallel execution
   - Measure wall-clock duration for each user (start to completion)
   - Measure total duration (all 3 complete)
3. Repeat test 3 times to establish baseline
4. Analyze scalability:
   - Per-user duration (concurrent vs sequential baseline)
   - Total duration (3 users concurrent vs 3 Ã— sequential)
   - Resource contention (CPU, memory, database locks)
   - Isolation effectiveness (per-user databases prevent conflicts?)
5. Document findings:
   - Average duration per user (concurrent scenario)
   - Total duration (all 3 complete)
   - Overhead quantified (concurrent vs 3 Ã— sequential)
   - Scalability assessment (linear, sub-linear, or super-linear)

**Evidence to capture**:
- Console logs showing concurrent execution (timestamps for each user)
- Final metrics (duration per user, total duration)
- Resource usage (CPU, memory, database I/O)

**Verification**:
- [ ] Test executed 3 times with consistent results
- [ ] Multi-user baseline documented
- [ ] Scalability assessment completed
- [ ] Optimization priority ranked (low if linear, high if super-linear overhead)

**Commit**: `test(15-02): establish P-4 multi-user concurrent baseline`

---

### Task 5: Document Performance Baselines and Optimization Priorities

**What**: Create comprehensive performance report with baselines and optimization roadmap.

**Steps**:
1. Synthesize findings from Tasks 1-4
2. Create `15-02-PERFORMANCE-REPORT.md` with structure:
   - Executive summary (key baselines, critical bottlenecks)
   - P-1 Customer Sync baseline (detailed metrics)
   - P-2 Product Sync baseline (image download impact analysis)
   - P-3 Delta Sync baseline (optimization validation)
   - P-4 Multi-User Concurrent baseline (scalability assessment)
   - Bottleneck prioritization matrix (impact Ã— frequency Ã— fix complexity)
   - Optimization roadmap for Phase 16+ (prioritized by ROI)
3. Update Phase 16+ scope based on performance findings
4. Identify quick wins (high impact, low complexity) vs long-term optimizations

**Verification**:
- [ ] Performance report created with all baselines
- [ ] Bottlenecks prioritized by impact
- [ ] Optimization roadmap defined
- [ ] Phase 16+ scope updated

**Commit**: `docs(15-02): create performance baseline report`

---

## Verification

After all tasks complete:

1. **Customer Sync Baseline (P-1)**: 15-20 min validated â†’ optimization LOW priority
2. **Product Sync Baseline (P-2)**: Image download impact quantified â†’ HIGH priority if blocking
3. **Delta Sync Baseline (P-3)**: Quick hash efficiency validated â†’ optimization working as intended
4. **Multi-User Concurrent Baseline (P-4)**: Scalability assessed â†’ MEDIUM priority if overhead significant

## Success Criteria

- [ ] P-1, P-2, P-3, P-4 tests executed 3 times each (12+ total runs)
- [ ] Performance baselines documented with metrics
- [ ] Bottlenecks identified and prioritized
- [ ] Optimization roadmap created for Phase 16+
- [ ] No code changes made (testing only, optimizations deferred)

## Output

**Artifacts**:
- `.planning/phases/15-individual-sync-testing/15-02-PERFORMANCE-REPORT.md` - Comprehensive performance report
- Baseline metrics for all 4 performance tests
- Bottleneck prioritization matrix
- Optimization roadmap for Phase 16+

**Knowledge gained**:
- Actual performance characteristics vs Phase 14 predictions
- Critical bottleneck: image downloads blocking product sync (validate Issue 4)
- Delta optimization effectiveness (quick hash savings validated)
- Multi-user scalability baseline (per-user isolation overhead)
