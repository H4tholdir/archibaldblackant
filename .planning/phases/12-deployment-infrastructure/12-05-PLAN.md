---
phase: 12-deployment-infrastructure
type: execute
---

<objective>
Implement automated backups, structured logging, and disaster recovery procedures.

Purpose: Ensure data safety with daily backups and enable troubleshooting with structured logs.
Output: Automated backup scripts, structured JSON logging, retention policy, disaster recovery documentation.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/12-deployment-infrastructure/12-CONTEXT.md
@archibald-web-app/backend/src/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create automated backup script</name>
  <files>scripts/backup.sh</files>
  <action>
    Create backup script at [scripts/backup.sh](scripts/backup.sh):

    ```bash
    #!/bin/bash
    set -euo pipefail

    # Automated Backup Script for Archibald Black Ant
    # Backs up SQLite databases and .env file
    # Usage: ./scripts/backup.sh

    BACKUP_DIR="/home/deploy/archibald-app/backups"
    DATA_DIR="/home/deploy/archibald-app/data"
    TIMESTAMP=$(date +%Y%m%d_%H%M%S)
    BACKUP_NAME="backup_${TIMESTAMP}.tar.gz"
    RETENTION_DAYS=7  # Keep last 7 days of backups

    echo "=== Archibald Black Ant Backup ==="
    echo "Starting backup at $(date)"

    # Create backup directory if not exists
    mkdir -p "$BACKUP_DIR"

    # Create temporary backup directory
    TMP_BACKUP="/tmp/archibald_backup_${TIMESTAMP}"
    mkdir -p "$TMP_BACKUP"

    # Copy SQLite databases
    echo "Backing up databases..."
    if [ -d "$DATA_DIR" ]; then
      cp -r "$DATA_DIR" "$TMP_BACKUP/data"
    else
      echo "Warning: Data directory not found at $DATA_DIR"
    fi

    # Copy .env file (contains critical configuration)
    echo "Backing up .env..."
    if [ -f "/home/deploy/archibald-app/.env" ]; then
      cp "/home/deploy/archibald-app/.env" "$TMP_BACKUP/.env"
    else
      echo "Warning: .env file not found"
    fi

    # Create compressed archive
    echo "Creating archive..."
    tar -czf "${BACKUP_DIR}/${BACKUP_NAME}" -C "$TMP_BACKUP" .

    # Cleanup temporary directory
    rm -rf "$TMP_BACKUP"

    # Set permissions (backup contains sensitive data)
    chmod 600 "${BACKUP_DIR}/${BACKUP_NAME}"

    # Get backup size
    BACKUP_SIZE=$(du -h "${BACKUP_DIR}/${BACKUP_NAME}" | cut -f1)
    echo "✓ Backup created: ${BACKUP_NAME} (${BACKUP_SIZE})"

    # Cleanup old backups (retention policy)
    echo "Cleaning up old backups (retention: ${RETENTION_DAYS} days)..."
    find "$BACKUP_DIR" -name "backup_*.tar.gz" -type f -mtime +${RETENTION_DAYS} -delete

    # Count remaining backups
    BACKUP_COUNT=$(ls -1 "$BACKUP_DIR"/backup_*.tar.gz 2>/dev/null | wc -l)
    echo "✓ ${BACKUP_COUNT} backups in retention window"

    echo "=== Backup Complete ==="
    echo "Backup path: ${BACKUP_DIR}/${BACKUP_NAME}"
    ```

    Make executable:
    ```bash
    chmod +x scripts/backup.sh
    ```

    Key decisions:
    - Daily backups with 7-day retention (balances safety vs. disk space)
    - Compressed tar.gz format (space-efficient, standard)
    - Timestamp naming (easy to identify and sort)
    - 600 permissions on backups (sensitive data - owner-only)
    - Automatic cleanup of old backups (prevent disk fill)
  </action>
  <verify>
    - [scripts/backup.sh](scripts/backup.sh) created
    - Script is executable
    - Backs up data/ directory (SQLite databases)
    - Backs up .env file (sensitive config)
    - Automatic cleanup after retention period
    - Backup permissions set to 600
  </verify>
  <done>
    Automated backup script created with 7-day retention and compression
  </done>
</task>

<task type="auto">
  <name>Task 2: Setup cron job for daily backups</name>
  <files>None (cron configuration on VPS)</files>
  <action>
    Document cron setup instructions for VPS (will be done during deployment):

    Create [scripts/setup-backup-cron.sh](scripts/setup-backup-cron.sh):

    ```bash
    #!/bin/bash
    # Setup daily backup cron job
    # Run once on VPS: ./scripts/setup-backup-cron.sh

    CRON_SCHEDULE="0 2 * * *"  # Daily at 2 AM
    BACKUP_SCRIPT="/home/deploy/archibald-app/scripts/backup.sh"
    LOG_FILE="/home/deploy/archibald-app/logs/backup.log"

    # Add cron job if not already present
    (crontab -l 2>/dev/null | grep -v "$BACKUP_SCRIPT"; echo "$CRON_SCHEDULE $BACKUP_SCRIPT >> $LOG_FILE 2>&1") | crontab -

    echo "✓ Daily backup cron job configured"
    echo "Schedule: Daily at 2 AM"
    echo "Logs: $LOG_FILE"

    # Test backup script
    echo "Running test backup..."
    "$BACKUP_SCRIPT"
    ```

    Make executable:
    ```bash
    chmod +x scripts/setup-backup-cron.sh
    ```

    Key decisions:
    - 2 AM daily schedule (low traffic time)
    - Logs to backup.log (troubleshooting)
    - Idempotent setup script (safe to re-run)
  </action>
  <verify>
    - [scripts/setup-backup-cron.sh](scripts/setup-backup-cron.sh) created
    - Script is executable
    - Cron schedule set to daily 2 AM
    - Backup logs redirected to logs/backup.log
  </verify>
  <done>
    Cron setup script created for daily automated backups
  </done>
</task>

<task type="auto">
  <name>Task 3: Create disaster recovery script</name>
  <files>scripts/restore.sh</files>
  <action>
    Create restore script at [scripts/restore.sh](scripts/restore.sh):

    ```bash
    #!/bin/bash
    set -euo pipefail

    # Disaster Recovery Script for Archibald Black Ant
    # Restores from backup archive
    # Usage: ./scripts/restore.sh <backup_file>

    if [ $# -eq 0 ]; then
      echo "Usage: ./scripts/restore.sh <backup_file>"
      echo ""
      echo "Available backups:"
      ls -lh /home/deploy/archibald-app/backups/backup_*.tar.gz 2>/dev/null || echo "No backups found"
      exit 1
    fi

    BACKUP_FILE="$1"
    RESTORE_DIR="/home/deploy/archibald-app"
    TMP_RESTORE="/tmp/archibald_restore_$$"

    echo "=== Archibald Black Ant Disaster Recovery ==="
    echo "Backup file: $BACKUP_FILE"

    # Verify backup file exists
    if [ ! -f "$BACKUP_FILE" ]; then
      echo "✗ Error: Backup file not found: $BACKUP_FILE"
      exit 1
    fi

    # Confirmation prompt
    echo ""
    echo "WARNING: This will restore data from backup and may overwrite current data."
    read -p "Are you sure you want to continue? (type 'yes' to confirm): " CONFIRM
    if [ "$CONFIRM" != "yes" ]; then
      echo "Restore cancelled."
      exit 0
    fi

    # Stop running containers
    echo "Stopping Docker containers..."
    cd "$RESTORE_DIR"
    docker compose down

    # Extract backup to temporary directory
    echo "Extracting backup..."
    mkdir -p "$TMP_RESTORE"
    tar -xzf "$BACKUP_FILE" -C "$TMP_RESTORE"

    # Restore data directory
    if [ -d "$TMP_RESTORE/data" ]; then
      echo "Restoring databases..."
      rm -rf "${RESTORE_DIR}/data"
      cp -r "$TMP_RESTORE/data" "${RESTORE_DIR}/data"
      echo "✓ Databases restored"
    else
      echo "⚠ Warning: No data directory in backup"
    fi

    # Restore .env file
    if [ -f "$TMP_RESTORE/.env" ]; then
      echo "Restoring .env..."
      cp "$TMP_RESTORE/.env" "${RESTORE_DIR}/.env"
      chmod 600 "${RESTORE_DIR}/.env"
      echo "✓ .env restored"
    else
      echo "⚠ Warning: No .env file in backup"
    fi

    # Cleanup temporary directory
    rm -rf "$TMP_RESTORE"

    # Restart containers
    echo "Restarting Docker containers..."
    docker compose up -d

    # Wait for health checks
    echo "Waiting for services to start..."
    sleep 30

    # Verify health
    if curl -sf http://localhost:3000/health > /dev/null 2>&1; then
      echo "✓ Health check passed"
    else
      echo "⚠ Warning: Health check failed, manual verification needed"
    fi

    echo "=== Restore Complete ==="
    echo "Please verify application functionality"
    ```

    Make executable:
    ```bash
    chmod +x scripts/restore.sh
    ```

    Key decisions:
    - Confirmation prompt (prevent accidental data loss)
    - Stops containers during restore (data consistency)
    - Automatic restart after restore (minimize downtime)
    - Health check verification (ensure successful restore)
    - 1-2 hour recovery time acceptable (per 12-CONTEXT.md)
  </action>
  <verify>
    - [scripts/restore.sh](scripts/restore.sh) created
    - Script is executable
    - Requires backup file path as argument
    - Confirmation prompt before restore
    - Stops/starts Docker containers
    - Health check verification after restore
  </verify>
  <done>
    Disaster recovery restore script created with safety confirmations
  </done>
</task>

<task type="auto">
  <name>Task 4: Implement structured JSON logging</name>
  <files>archibald-web-app/backend/src/logger.ts</files>
  <action>
    Create structured logger at [archibald-web-app/backend/src/logger.ts](archibald-web-app/backend/src/logger.ts):

    ```typescript
    // Structured JSON Logger for Production
    // Per 12-CONTEXT.md: timestamps, user IDs, request IDs

    export interface LogContext {
      timestamp?: string;
      userId?: string;
      requestId?: string;
      [key: string]: any;
    }

    export type LogLevel = 'debug' | 'info' | 'warn' | 'error';

    class Logger {
      private env: string;

      constructor() {
        this.env = process.env.NODE_ENV || 'development';
      }

      private formatLog(level: LogLevel, message: string, context?: LogContext): string {
        const logEntry = {
          level,
          message,
          timestamp: new Date().toISOString(),
          env: this.env,
          ...context,
        };

        if (this.env === 'production') {
          // JSON format for production (easy to parse with log aggregators)
          return JSON.stringify(logEntry);
        } else {
          // Human-readable for development
          return `[${logEntry.timestamp}] ${level.toUpperCase()}: ${message} ${context ? JSON.stringify(context) : ''}`;
        }
      }

      debug(message: string, context?: LogContext) {
        if (this.env !== 'production') {
          console.log(this.formatLog('debug', message, context));
        }
      }

      info(message: string, context?: LogContext) {
        console.log(this.formatLog('info', message, context));
      }

      warn(message: string, context?: LogContext) {
        console.warn(this.formatLog('warn', message, context));
      }

      error(message: string, context?: LogContext) {
        console.error(this.formatLog('error', message, context));
      }

      // Helper: Create child logger with persistent context (e.g., requestId)
      child(persistentContext: LogContext): Logger {
        const childLogger = new Logger();
        const originalMethods = {
          debug: childLogger.debug.bind(childLogger),
          info: childLogger.info.bind(childLogger),
          warn: childLogger.warn.bind(childLogger),
          error: childLogger.error.bind(childLogger),
        };

        childLogger.debug = (message, context) => originalMethods.debug(message, { ...persistentContext, ...context });
        childLogger.info = (message, context) => originalMethods.info(message, { ...persistentContext, ...context });
        childLogger.warn = (message, context) => originalMethods.warn(message, { ...persistentContext, ...context });
        childLogger.error = (message, context) => originalMethods.error(message, { ...persistentContext, ...context });

        return childLogger;
      }
    }

    export const logger = new Logger();
    ```

    Integrate into backend in [archibald-web-app/backend/src/index.ts](archibald-web-app/backend/src/index.ts):

    ```typescript
    import { logger } from './logger';
    import { v4 as uuidv4 } from 'uuid';

    // Middleware: Add requestId to all requests
    app.use((req, res, next) => {
      const requestId = uuidv4();
      (req as any).requestId = requestId;
      (req as any).logger = logger.child({ requestId, userId: (req as any).user?.userId });
      next();
    });

    // Replace all console.log with logger
    // Example:
    // OLD: console.log('User logged in:', userId);
    // NEW: req.logger.info('User logged in', { userId });
    ```

    Key decisions:
    - JSON format in production (structured, parseable)
    - Human-readable in development (easier debugging)
    - Child logger pattern (attach requestId/userId to all logs)
    - No external dependencies (pino/winston add complexity)
    - Timestamps, requestId, userId per 12-CONTEXT.md requirements
  </action>
  <verify>
    - [archibald-web-app/backend/src/logger.ts](archibald-web-app/backend/src/logger.ts) created
    - Logger exports debug, info, warn, error methods
    - JSON format in production, readable in development
    - child() method creates logger with persistent context
    - TypeScript compiles without errors
  </verify>
  <done>
    Structured JSON logger implemented with requestId and userId tracking
  </done>
</task>

<task type="auto">
  <name>Task 5: Document disaster recovery procedures</name>
  <files>.planning/phases/12-deployment-infrastructure/DISASTER-RECOVERY.md</files>
  <action>
    Create disaster recovery documentation:

    ```markdown
    # Disaster Recovery Procedures

    ## Overview

    Archibald Black Ant uses automated daily backups with 7-day retention. Recovery time objective (RTO): 1-2 hours.

    ## Backup Schedule

    - **Frequency**: Daily at 2 AM (automatic via cron)
    - **Retention**: Last 7 days
    - **Location**: `/home/deploy/archibald-app/backups/`
    - **Format**: Compressed tar.gz archives
    - **Contents**: SQLite databases (data/), .env file

    ## Recovery Scenarios

    ### Scenario 1: Database Corruption

    **Symptoms**: App unable to read/write data, SQLite errors in logs

    **Steps**:
    1. SSH to VPS: `ssh deploy@<VPS_IP>`
    2. List available backups: `ls -lh ~/archibald-app/backups/`
    3. Choose recent backup (e.g., `backup_20260117_020000.tar.gz`)
    4. Run restore script: `~/archibald-app/scripts/restore.sh ~/archibald-app/backups/backup_20260117_020000.tar.gz`
    5. Confirm restoration when prompted (type `yes`)
    6. Wait ~5-10 minutes for restore and container restart
    7. Verify: Visit https://archibaldblackant.it and test login + order creation

    **RTO**: 15-30 minutes

    ### Scenario 2: Complete VPS Failure

    **Symptoms**: VPS unreachable, hosting provider outage

    **Steps**:
    1. Provision new VPS (same specs: 2 vCPU / 4 GB RAM)
    2. Run Phase 12 Plan 1 (VPS setup, Docker, DNS)
    3. Run Phase 12 Plan 2 (Docker orchestration, SSL)
    4. Download latest backup from failed VPS (if accessible) OR use off-site backup (if implemented)
    5. Upload backup to new VPS: `scp backup_*.tar.gz deploy@<NEW_VPS_IP>:~/archibald-app/backups/`
    6. Run restore script on new VPS
    7. Update DNS A record to point to new VPS IP
    8. Wait 5-10 minutes for DNS propagation
    9. Verify: Visit https://archibaldblackant.it

    **RTO**: 1-2 hours (includes VPS provisioning, DNS propagation)

    ### Scenario 3: Accidental Data Deletion

    **Symptoms**: User reports missing orders/customers, data inconsistencies

    **Steps**:
    1. Identify when data was last correct (approximate timestamp)
    2. Choose backup before deletion occurred
    3. Run restore script (same as Scenario 1)
    4. Alternative: Extract specific database file without full restore:
       ```bash
       mkdir /tmp/partial_restore
       tar -xzf ~/archibald-app/backups/backup_*.tar.gz -C /tmp/partial_restore
       cp /tmp/partial_restore/data/customers.db ~/archibald-app/data/customers.db
       docker compose restart backend
       ```

    **RTO**: 10-20 minutes

    ## Testing Recovery

    **Test procedure** (run quarterly):
    1. Create test backup: `~/archibald-app/scripts/backup.sh`
    2. Note current database record count (e.g., total orders)
    3. Run restore with test backup: `~/archibald-app/scripts/restore.sh ~/archibald-app/backups/backup_<timestamp>.tar.gz`
    4. Verify record count matches pre-restore state
    5. Test app functionality (login, order creation)

    ## Off-Site Backup (Optional Enhancement)

    Current backups stored on VPS only. For additional safety, consider:
    - AWS S3 (encrypted bucket, lifecycle policy for 30-day retention)
    - Rsync to separate server (daily cron job)
    - Manual download to local machine (weekly via `scp`)

    ## Backup Monitoring

    Check backup logs regularly:
    ```bash
    tail -50 ~/archibald-app/logs/backup.log
    ```

    Expected output: Daily successful backups with file size ~5-50 MB.

    ## Emergency Contacts

    - VPS Provider Support: [provider support URL]
    - DNS Provider Support: [registrar support URL]
    - GitHub Repository: https://github.com/[username]/archibald-black-ant

    ## Recovery Checklist

    - [ ] Identify failure scenario
    - [ ] SSH access to VPS working (or provision new VPS)
    - [ ] Locate most recent valid backup
    - [ ] Run restore script with confirmation
    - [ ] Wait for container restart and health checks
    - [ ] Verify application functionality
    - [ ] Notify users of restoration (if significant downtime)
    - [ ] Document incident and lessons learned
    ```
  </action>
  <verify>
    - [.planning/phases/12-deployment-infrastructure/DISASTER-RECOVERY.md](.planning/phases/12-deployment-infrastructure/DISASTER-RECOVERY.md) created
    - Documents 3 recovery scenarios with RTOs
    - Step-by-step procedures included
    - Testing procedure documented
    - Off-site backup recommendations included
  </verify>
  <done>
    Disaster recovery procedures documented with RTO targets and testing guidance
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Automated backup script created and executable
- [ ] Backup retention set to 7 days
- [ ] Cron setup script created for daily 2 AM backups
- [ ] Disaster recovery restore script created and executable
- [ ] Structured JSON logger implemented
- [ ] Logger integrated into backend with requestId middleware
- [ ] Disaster recovery documentation complete
- [ ] All scripts have proper permissions (executable, 600 for sensitive)
</verification>

<success_criteria>

- All tasks completed
- Backups automated with retention policy
- Disaster recovery procedures documented
- Structured logging implemented
- No errors in verification checks
</success_criteria>

<output>
After completion, create `.planning/phases/12-deployment-infrastructure/12-05-SUMMARY.md`
</output>
